================================================================================
AFTERLINK - AI IMPLEMENTATION & QUALITY ASSURANCE DOCUMENTATION
================================================================================

This document explains every AI integration in the Afterlink platform, the logic
behind each implementation, and why the outputs can be trusted for quality.

================================================================================
TABLE OF CONTENTS
================================================================================

1. Overview of AI Strategy
2. Search & Discovery
   2.1 Query Validation
   2.2 Article Relevance Matching
   2.3 New Article Suggestions
3. Article Generation
   3.1 Full Article with Embedded Questions
   3.2 Article Snippet Generation
4. Conversational Lead Qualification
   4.1 First Message Handler
   4.2 Follow-up Message Handler
   4.3 Lead Profile Extraction
5. ICP Matching & Scoring
   5.1 ICP Criteria Extraction
   5.2 Lead Attribute Extraction
   5.3 Deterministic Scoring Algorithm
6. Quality Assurance Summary

================================================================================
1. OVERVIEW OF AI STRATEGY
================================================================================

Afterlink uses a HYBRID approach that combines:
- LLM capabilities for understanding natural language and extracting structure
- Deterministic algorithms for consistent, auditable scoring
- Strict prompts with explicit rules to minimize hallucination

CORE PRINCIPLE: Use AI for what it's good at (understanding intent, extracting
structure from unstructured text) and use code for what requires consistency
(scoring, validation, business logic).

================================================================================
2. SEARCH & DISCOVERY
================================================================================

--------------------------------------------------------------------------------
2.1 QUERY VALIDATION
--------------------------------------------------------------------------------

PURPOSE: Determine if a user's search query is comprehensible/meaningful

IMPLEMENTATION:
- Simple yes/no classification task
- Prompt asks: "Is this a comprehensible search query?"
- Response constrained to "yes" or "no"

WHY THIS IS RELIABLE:
- Binary classification is one of the simplest LLM tasks
- No generation required, just judgment
- Low hallucination risk due to constrained output
- Fallback: If unclear, defaults to treating as valid (user-friendly)

LOCATION: search.ts, lines 38-47

--------------------------------------------------------------------------------
2.2 ARTICLE RELEVANCE MATCHING
--------------------------------------------------------------------------------

PURPOSE: Given existing articles, determine which are relevant to a search query

IMPLEMENTATION:
- Input: List of existing articles (id, title, snippet)
- Input: User's search query
- Output: JSON array of relevant article IDs

WHY THIS IS RELIABLE:
- Structured input (article list with clear fields)
- Structured output (array of integers)
- LLM only needs to match concepts, not generate content
- Deduplication applied to output (code-level safeguard)
- JSON parsing validates output format

LOCATION: search.ts, lines 74-105

--------------------------------------------------------------------------------
2.3 NEW ARTICLE SUGGESTIONS
--------------------------------------------------------------------------------

PURPOSE: Generate new article ideas when not enough existing articles match

IMPLEMENTATION:
- Input: Search query, list of existing titles to avoid
- Output: JSON array of {title, snippet} objects
- Constrained to generate exactly N suggestions

WHY THIS IS RELIABLE:
- Explicit instruction to avoid existing titles (prevents duplicates)
- Structured JSON output is validated by parsing
- Short outputs (title + snippet) reduce hallucination surface
- Suggestions are just titles - actual content generated separately

LOCATION: search.ts, lines 112-142

================================================================================
3. ARTICLE GENERATION
================================================================================

--------------------------------------------------------------------------------
3.1 FULL ARTICLE WITH EMBEDDED QUESTIONS
--------------------------------------------------------------------------------

PURPOSE: Generate informative articles with strategically placed questions that
encourage readers to share personal information

IMPLEMENTATION:
- Input: Article title
- Output: 5-7 paragraph article (500-600 words)
- Special format: {{Q:question text}} markers for interactive questions

PROMPT DESIGN RATIONALE:

1. QUESTION PLACEMENT RULES:
   - Questions must directly relate to preceding content
   - Questions ask for personalization of general information
   - First-person format ("I", "my", "me")
   - Article must NOT answer the question after it

   WHY: This ensures questions feel natural and create genuine curiosity
   gaps that motivate sharing personal context.

2. EXAMPLE-DRIVEN LEARNING:
   - Prompt includes concrete examples of good question placement
   - Shows the pattern: general statement → personalization question

   WHY: Examples are more effective than abstract rules for LLMs.
   They demonstrate the exact pattern we want.

3. WRITING STYLE CONSTRAINTS:
   - Conversational tone
   - Hook-based opening
   - Use of "you" and "your"

   WHY: Consistent style makes articles feel cohesive and engaging.

WHY THIS IS RELIABLE:
- Structured output format ({{Q:...}}) is easily validated
- Content length is constrained (500-600 words)
- Examples in prompt guide the model toward desired behavior
- Questions are validated by regex parsing on the client

LOCATION: search.ts, lines 177-207

--------------------------------------------------------------------------------
3.2 ARTICLE SNIPPET GENERATION
--------------------------------------------------------------------------------

PURPOSE: Create one-sentence summary for article listings

IMPLEMENTATION:
- Input: Article title
- Output: Max 150 characters
- Constraint: Just the summary, no quotes or formatting

WHY THIS IS RELIABLE:
- Extremely constrained output (one sentence, 150 chars)
- Simple summarization task
- Length limit enforced in prompt

LOCATION: search.ts, lines 211-215

================================================================================
4. CONVERSATIONAL LEAD QUALIFICATION
================================================================================

--------------------------------------------------------------------------------
4.1 FIRST MESSAGE HANDLER
--------------------------------------------------------------------------------

PURPOSE: When a user clicks a question, respond by asking for the specific
personal information implied by their question

IMPLEMENTATION:
- Input: Article title, paragraph context, question clicked
- Output: 2-3 sentence response asking for relevant personal info

PROMPT DESIGN RATIONALE:

1. CRITICAL INSIGHT EXPLANATION:
   The prompt explicitly explains: "The question they clicked REVEALS what
   personal information they're open to sharing."

   Examples provided:
   - "What fits my budget?" → Ask about BUDGET
   - "Would this work for my age?" → Ask about AGE

   WHY: This creates a consistent mapping between question type and
   information to request.

2. STRICT RULES:
   - ONLY ask about what their question implies
   - Do NOT ask about unrelated personal details
   - Do NOT answer the question yet

   WHY: Prevents the AI from being overly invasive or off-topic.

3. SESSION MEMORY INTEGRATION:
   - Previous insights from this session are injected
   - Instruction: "Don't ask about things they already told you"

   WHY: Creates continuity and avoids annoying repetition.

WHY THIS IS RELIABLE:
- Clear mapping from question type to response type
- Examples ground the model's behavior
- Short output (2-3 sentences) limits variability
- Session memory prevents redundant questions

LOCATION: search.ts, lines 358-388

--------------------------------------------------------------------------------
4.2 FOLLOW-UP MESSAGE HANDLER
--------------------------------------------------------------------------------

PURPOSE: Respond helpfully to user messages after initial exchange

IMPLEMENTATION:
- Input: Full conversation history, article context, user's message
- Output: Helpful response (2-3 sentences)

PROMPT DESIGN RATIONALE:

1. RESPONSE-FOCUSED:
   - "Simply answer or respond to what they said"
   - "Be helpful and warm"

2. STRICT RULES:
   - Do NOT ask follow-up questions (unless truly necessary)
   - Do NOT introduce new topics
   - Do NOT probe for more information
   - Just respond, then stop

   WHY: Prevents the AI from being pushy or interrogative. The lead
   extraction happens separately - the conversation should feel natural.

WHY THIS IS RELIABLE:
- Clear instruction to NOT probe further
- Conversation history provides context for coherent responses
- Short output constraint
- Separation of concerns: conversation vs. extraction

LOCATION: search.ts, lines 497-522

--------------------------------------------------------------------------------
4.3 LEAD PROFILE EXTRACTION
--------------------------------------------------------------------------------

PURPOSE: Extract factual lead information from conversation history

IMPLEMENTATION:
- Input: All user messages from the conversation
- Input: Any existing notes from this session
- Output: JSON {theme, note}

PROMPT DESIGN RATIONALE:

1. SALESMAN FRAMING:
   "You are helping a salesman prepare for outreach. Write a factual profile."

   WHY: This framing encourages practical, actionable information extraction.

2. STRICT FACTUAL RULES:
   - ONLY include facts the lead EXPLICITLY stated
   - DO NOT guess, infer, or assume anything
   - DO NOT estimate demographics unless stated
   - If they said "I spend $5-8" write exactly that

   WHY: Prevents hallucination of lead information. Only real data is stored.

3. INTEREST VS CURIOSITY DISTINCTION:
   - Asking questions is NOT interest, just curiosity
   - ONLY note interest if explicitly stated:
     - "I want to..." / "I'd like to..."
     - "I plan to..." / "I'm going to..."
     - "I need..." / "I'm trying to..."
   - Questions like "How does X work?" = curiosity, NOT interest

   WHY: This critical distinction prevents over-interpretation of user
   behavior. A question shows curiosity, not buying intent.

4. EMPTY OUTPUT HANDLING:
   "If they only asked questions without stating facts, return empty note."

   WHY: Better to have no data than hallucinated data.

WHY THIS IS RELIABLE:
- Explicit rules against inference
- Clear distinction between curiosity and interest
- Structured JSON output validates format
- Empty output is acceptable (no pressure to fabricate)
- Input is ONLY user messages (not article content)

LOCATION: search.ts, lines 415-446

================================================================================
5. ICP MATCHING & SCORING (B2C Lead Scoring)
================================================================================

This system uses a B2C-specific lead scoring approach based on industry
frameworks (adapted from BANT, RFM, and behavioral scoring for consumers).

Unlike B2B frameworks that focus on company size, authority, and decision
processes, B2C scoring focuses on individual consumer behavior, spending
patterns, and purchase intent signals.

--------------------------------------------------------------------------------
5.1 THE 5 B2C SCORING DIMENSIONS
--------------------------------------------------------------------------------

| Dimension   | Weight | What It Measures                              |
|-------------|--------|-----------------------------------------------|
| FIT         | 35%    | Conceptual overlap with ICP's world/domain    |
| BUDGET      | 20%    | Spending capacity matches ICP expectations    |
| NEED        | 20%    | Has a problem/desire the ICP addresses        |
| URGENCY     | 15%    | Active buying signals vs passive browsing     |
| ENGAGEMENT  | 10%    | How much useful detail they shared            |

WHY THESE DIMENSIONS:
- Based on B2C lead scoring research (not B2B frameworks like BANT/MEDDIC)
- FIT is weighted highest because irrelevant leads waste resources
- BUDGET and NEED are equal - both critical for conversion
- URGENCY differentiates hot leads from cold ones
- ENGAGEMENT indicates data quality and lead seriousness

--------------------------------------------------------------------------------
5.2 CONCEPTUAL MATCHING (The Key Innovation)
--------------------------------------------------------------------------------

PURPOSE: Score FIT based on conceptual/semantic overlap, not literal word matching

THE PROBLEM WITH LITERAL MATCHING:
- "fine lines" and "wrinkles" are the same concept but different words
- "bubble tea" is a "sweet beverage" but substring matching fails
- "anti-aging products" relates to "aging concerns" conceptually

THE SOLUTION: CONCEPTUAL REASONING

The LLM is instructed to think in THREE STEPS:

STEP 1: UNDERSTAND THE ICP
- What domain/category? (beauty, food, fitness, finance, etc.)
- What underlying concerns? (aging, health, saving money, etc.)
- What type of person? (high spender, budget-conscious, etc.)

STEP 2: UNDERSTAND THE LEAD
- What domain are they in?
- What are their underlying concerns?
- What type of person are they?

STEP 3: SCORE CONCEPTUALLY
- Think about underlying domains, not surface words
- Would a product for the ICP plausibly interest this lead?
- Are they in the same "world" of concerns?

EXAMPLE:
ICP: "Aging people discovering wrinkles, willing to spend a lot"
Lead: "Concerned about fine lines. $150/month on skincare. Looking for anti-aging."

Conceptual analysis:
- ICP domain: beauty/skincare, aging concerns, high spenders
- Lead domain: beauty/skincare, aging concerns (fine lines = wrinkles), high spender
- Result: HIGH FIT (same world, same concerns, same spending type)

WHY THIS IS RELIABLE:
- LLM excels at understanding conceptual relationships
- No hardcoded examples needed - works across all domains
- Explicit instruction to think conceptually, not literally
- Still constrained by structured output (0-100 score + reason)

--------------------------------------------------------------------------------
5.3 SCORING METHODOLOGY
--------------------------------------------------------------------------------

SINGLE LLM CALL PER LEAD:
- Input: ICP description + Lead profile
- Output: JSON with 5 dimension scores (0-100 each) + reasons
- Each dimension scored independently

DIMENSION SCORING RULES:

FIT (0-100):
- 100 = Perfect domain match, same world of concerns
- 50-80 = Related domain, some overlap
- 0 = Completely different worlds (piano vs beverages)

BUDGET (0-100):
- Score based on match between lead's spending and ICP expectations
- High-spender ICP + high-spending lead = high score
- Budget-friendly ICP + budget-conscious lead = high score
- 0 = No spending information mentioned

NEED (0-100):
- Look for: "concerned about", "want to", "looking for", "struggling with"
- Need must be relevant to what ICP is solving
- 0 = No problems or desires expressed

URGENCY (0-100):
- High: "looking for", "need", "planning to buy"
- Medium: "interested in", "considering"
- Low/Zero: Just describing habits

ENGAGEMENT (0-100):
- High: Multiple specific preferences, quantities, frequencies
- Medium: Some details
- Low: Vague statements

STRICT RULES IN PROMPT:
- Score based ONLY on explicitly stated information
- 0 means NO relevant information (not "unknown")
- Think conceptually about FIT, strictly about other dimensions
- No hallucination or inference beyond what's stated

--------------------------------------------------------------------------------
5.4 FINAL SCORE CALCULATION
--------------------------------------------------------------------------------

The final score uses FIT as a GATE to prevent high scores for irrelevant leads.

FORMULA:
```
fitGate = fit^1.5  (exponential curve)
rawTotal = fit_points + budget_points + need_points + urgency_points + engagement_points
finalScore = rawTotal × fitGate
```

WHY EXPONENTIAL FIT GATE:
- Low FIT should crush the entire score
- A lead with perfect demographics but wrong domain is worthless
- Prevents gaming by having high scores in non-FIT dimensions

EXAMPLES:
| Lead Type              | FIT | Other Dims | fitGate | Final |
|------------------------|-----|------------|---------|-------|
| Perfect match          | 90% | 80 pts     | 0.85    | ~68   |
| Good match             | 70% | 70 pts     | 0.59    | ~41   |
| Wrong domain           | 20% | 90 pts     | 0.09    | ~8    |

--------------------------------------------------------------------------------
5.5 ANTI-HALLUCINATION MEASURES
--------------------------------------------------------------------------------

1. EXPLICIT SCORING RULES:
   - "Score based ONLY on what is explicitly stated"
   - "0 means NO relevant information, not unknown"
   - "Do NOT give benefit of the doubt"

2. STRUCTURED OUTPUT:
   - JSON format validated by parsing
   - Each dimension requires a score AND a reason
   - Reasons must explain the conceptual connection

3. CONCEPTUAL BUT BOUNDED:
   - FIT uses conceptual matching (LLM strength)
   - Other dimensions use explicit criteria (reduces hallucination)
   - "Be generous with FIT if domains overlap, strict with other dimensions"

4. NO HARDCODED EXAMPLES:
   - Prompt teaches conceptual thinking, not specific mappings
   - Works across all domains without maintenance
   - LLM applies same reasoning to any ICP/lead combination

LOCATION: search.ts, lines 691-880

================================================================================
6. QUALITY ASSURANCE SUMMARY
================================================================================

STRATEGY 1: CONSTRAINED OUTPUTS
- Binary yes/no for validation
- Short text for snippets (150 chars)
- Structured JSON for data extraction
- 2-3 sentence limits for responses
- Each score requires a reason/explanation

STRATEGY 2: EXPLICIT RULES
- "ONLY include explicitly stated facts"
- "DO NOT guess or infer"
- "Questions are curiosity, NOT interest"
- "0 means NO information, not unknown"
- "Score based ONLY on what is explicitly stated"

STRATEGY 3: CONCEPTUAL WHERE APPROPRIATE
- FIT dimension uses conceptual/semantic matching (LLM strength)
- Other dimensions use strict explicit criteria
- No hardcoded domain-specific examples
- LLM taught to think conceptually, not literally

STRATEGY 4: MATHEMATICAL GATING
- FIT acts as exponential gate on final score
- Low FIT crushes score regardless of other dimensions
- Prevents irrelevant leads from scoring high
- Formula: finalScore = rawTotal × fit^1.5

STRATEGY 5: TRANSPARENCY
- Every score has a 5-dimension breakdown
- Every dimension shows points/max + reason
- Reasons explain the conceptual connection
- Strong/weak points summarized

STRATEGY 6: VALIDATION LAYERS
- JSON parsing validates structure
- Regex validates embedded questions
- Code deduplicates results
- Each dimension independently scored

--------------------------------------------------------------------------------
RELIABILITY CONCLUSION
--------------------------------------------------------------------------------

The AI integrations in Afterlink are designed with the following principles:

1. Use AI for conceptual understanding (FIT matching)
2. Use strict rules for factual extraction (other dimensions)
3. Mathematical formulas ensure consistency (exponential gating)
4. Structured outputs enable validation
5. Transparency enables auditing (reasons for every score)
6. Zero scores for missing data (not 50%, not "unknown")

These design choices ensure that:
- Lead data is factual, not inferred
- FIT is evaluated conceptually (works across all domains)
- Other dimensions are evaluated strictly (no hallucination)
- Irrelevant leads are filtered out by exponential FIT gate
- Business decisions can be justified with specific criteria
- The system works for any ICP without hardcoded mappings

================================================================================
END OF DOCUMENTATION
================================================================================
